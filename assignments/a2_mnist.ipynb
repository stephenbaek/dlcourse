{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Iowa_IE6380_Assignment_01.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "source": [
        "<a href=\"https://colab.research.google.com/github/stephenbaek/dlcourse/blob/main/assignments/a2_mnist.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whV1ue6Nc1NC"
      },
      "source": [
        "# Assignment 2\n",
        "\n",
        "In this assignment, you will be implementing a Perceptron hand-written digit classifier using TensorFlow by answering the following questions."
      ]
    },
    {
      "source": [
        "### Background\n",
        "\n",
        "The [Modified NIST (MNIST) Database](http://yann.lecun.com/exdb/mnist/) is a database of 70,000 hand-written digits, which is a subset of a larger set available from the [National Institute of Standards and Technology (NIST)](https://www.nist.gov/srd/nist-special-database-19). The MNIST dataset is comprised of 60,000 training images and 10,000 test images, each of which are size-normalized to a 28-by-28-pixel image (see https://en.wikipedia.org/wiki/MNIST_database for more).\n",
        "\n",
        "TensorFlow comes with an MNIST helper code under [`tf.keras.datasets`](https://www.tensorflow.org/api_docs/python/tf/keras/datasets) module, which facilitates loading and parsing MNIST images and labels:"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MujL7IsfipmA"
      },
      "source": [
        "import tensorflow as tf\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "print('train images shape:', train_images.shape )\n",
        "print('train labels:', len(train_labels))\n",
        "print(train_labels)\n",
        "print('test images shape:', test_images.shape)\n",
        "print('test labels:', len(test_labels))\n",
        "print(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
        "    plt.xlabel(train_labels[i])\n",
        "plt.show()"
      ]
    },
    {
      "source": [
        "### Question 1.\n",
        "\n",
        "Use `tf.data.Dataset.from_tensor_slices()` to create `tf.data.Dataset` objects named `train_image_ds`, `train_label_ds`, `test_image_ds`, and `test_label_ds` from the MNIST dataset above. (Hint: https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices)"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PROVIDE YOUR ANSWER HERE"
      ]
    },
    {
      "source": [
        "### Question 2.\n",
        "\n",
        "Consolidate `train_image_ds` and `train_label_ds` to build `train_ds` using `tf.data.Dataset.zip()` (https://www.tensorflow.org/api_docs/python/tf/data/Dataset#zip). Repeat the same for `test_image_ds` and `test_label_ds` to build `test_ds`."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PROVIDE YOUR ANSWER HERE"
      ]
    },
    {
      "source": [
        "### Checkpoint 1.\n",
        "If your answers to the above two questions were correct, you should be able to run the following code to visualize the first 25 training images:"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i, (image, label) in enumerate(train_ds.take(25)):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(image, cmap=plt.cm.binary)\n",
        "    plt.xlabel(label.numpy())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA5G5pDplAt1"
      },
      "source": [
        "### Question 3.\n",
        "\n",
        "As like many other image datasets, MNIST datasets originally have pixel values ranging from 0 to 255. Write a function named `normalize()` to normalize the pixel values, so that they range between $[0, 1]$.\n",
        "\n",
        "Hint:\n",
        "- `normalize` function must take two arguments, `image` and `label`.\n",
        "- Note that `image` is originally of data type `tf.uint8` (8-bit unsigned integers). This must be casted to `tf.float32` before to be normalized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7YgFvrGlC89"
      },
      "source": [
        "# PROVIDE YOUR ANSWER HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "### Question 4.\n",
        "\n",
        "The raw MNIST data comes with integer labels, from 0 to 9. Implement a function named `one_hot()` to convert the integer labels to probability labels (one-hot encoding).\n",
        "\n",
        "Hint:\n",
        "- `one_hot` function must expect two arguments, `image` and `label`.\n",
        "- You can either write your own code or use `tf.one_hot` (https://www.tensorflow.org/api_docs/python/tf/one_hot)."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PROVIDE YOUR ANSWER HERE"
      ]
    },
    {
      "source": [
        "### Question 5.\n",
        "Images in the MNIST dataset are size-normalized to `(28,28)` rank-2 tensors. Later on, we will need to \"flatten\" them to rank-1 tensors of dimension 784=28*28. Implement a function named `flatten()` to perform such an operation.\n",
        "\n",
        "Hint:\n",
        "- `flatten` function must expect two arguments, `image` and `label`.\n",
        "- `tf.reshape` function is available in TensorFlow (https://www.tensorflow.org/api_docs/python/tf/reshape)."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PROVIDE YOUR ANSWER HERE"
      ]
    },
    {
      "source": [
        "### Question 6.\n",
        "\n",
        "Apply `normalize()`, `one_hot()`, and `flatten()` functions to `train_ds` using `tf.data.Dataset.map()` (https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map). Repeat the same for `test_ds`. Reuse the names `train_ds` and `test_ds` for the normalized datasets."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PROVIDE YOUR ANSWER HERE"
      ]
    },
    {
      "source": [
        "### Question 7.\n",
        "\n",
        "Randomly shuffle the elements of `train_ds` and divide them into batches of size 32. Divide the elements of `test_ds` into batches of the same size. You don't need to shuffle `test_ds`. Hint: https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle and https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PROVIDE YOUR ANSWER HERE"
      ]
    },
    {
      "source": [
        "### Checkpoint 2.\n",
        "\n",
        "If your answers to the questions above were correct, the following code should produce a congratulations message."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for ds in [train_ds, test_ds]:\n",
        "    for image_batch, label_batch in ds.shuffle(1024).take(10):   # testing only randomly selected subsets to save time\n",
        "        assert tf.reduce_all(tf.shape(image_batch) == (32, 784,))\n",
        "        assert tf.reduce_max(image_batch) <= 1\n",
        "        assert tf.reduce_min(image_batch) >= 0\n",
        "        assert tf.reduce_all(tf.shape(label_batch) == (32, 10,))\n",
        "        assert tf.reduce_sum(label_batch) == 32\n",
        "        assert tf.reduce_max(label_batch) == 1\n",
        "        assert tf.reduce_min(label_batch) == 0\n",
        "print(\"Congratulations! You passed the second checkpoint.\")"
      ]
    },
    {
      "source": [
        "### Question 8.\n",
        "\n",
        "A perceptron model can be written as $f(X \\mid W, b) = \\sigma(XW + b)$ where $W$ and $b$ are model paramters and $\\sigma$ is the sigmoid function. In our case, $X$ is a flattened image of the dimension 784. Given this, what must be the rank and dimensions for tensors $W$ and $b$? Implement a code to initialize `W` and `b` with their respective sizes."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5NI_yUDMNnp"
      },
      "source": [
        "# PROVIDE YOUR ANSWER HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "### Question 9.\n",
        "Now, define a function named `perceptron()` which takes a `(-1, 784)` tensor `X` as an input and returns the evaluation of $\\sigma(XW + b)$ as an output. Note that the expected output of `perceptron()` is class probabilities implying the predicted probability of `X` falling under each class. Hint: the sigmoid function $\\sigma$ is available as `tf.sigmoid()` in TensorFlow."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PROVIDE YOUR ANSWER HERE"
      ]
    },
    {
      "source": [
        "### Question 10.\n",
        "\n",
        "Define a function `accuracy()` which takes a one-hot encoded ground truth `y` and a perceptron prediction `f` as inputs and returns the percentage of correct predictions out of all predictions. Remember that both `y` and `f` are probability valued, to which `tf.argmax()` might be useful. The agreement between ground truth labels and predicted labels could be compared using `tf.equal` or `==` operator. Finally, note that you are expected to return the \"average of correctness\", to which `tf.reduce_mean()` might be of use. "
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PROVIDE YOUR ANSWER HERE"
      ]
    },
    {
      "source": [
        "### Question 11.\n",
        "Define `cross_entropy()` function, which takes the same set of inputs as the `accuracy()` function above, but returns the average cross entropy. Do not use off-the-shelf cross entropy functions available in TensorFlow but implement it from scratch."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PROVIDE YOUR ANSWER HERE"
      ]
    },
    {
      "source": [
        "### Question 12.\n",
        "\n",
        "Define a function named `gradient_descent()` to update `W` and `b` by one step using gradients produced from `cross_entropy()` function above.\n",
        "\n",
        "Note:\n",
        "- `gradient_descent()` must expect three inputs, namely `X`, `y`, and `learning_rate`.\n",
        "- `X` is a tensor containing a flattened image or a batch of flattened images.\n",
        "- `y` is a tensor containing probability-encoded labels corresponding to `X`.\n",
        "- `learning_rate` is a floating point value.\n",
        "- `gradient_descent()` is expected to return two values, `loss, acc`, corresponding to cross entropy and accuracy, respectively."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PROVIDE YOUR ANSWER HERE"
      ]
    },
    {
      "source": [
        "### Question 13.\n",
        "\n",
        "Implement `evaluate()` function that takes a batch dataset `ds` and evaluates the average loss and the average accuracy of the perceptron across `ds`."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PROVIDE YOUR ANSWER HERE"
      ]
    },
    {
      "source": [
        "### Checkpoint 3 (Final)\n",
        "\n",
        "If you were successful implementing the above, you should be able to run the training code below."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "!pip install -q tqdm\n",
        "from tqdm import tqdm # Progress bar\n",
        "import time\n",
        "\n",
        "learning_rate = 1e-3\n",
        "for epoch in range(10):\n",
        "    cnt = 0\n",
        "    total_acc = 0.0\n",
        "    total_loss = 0.0\n",
        "    for X, y in tqdm(train_ds, desc=f'Epoch {epoch:03d}'):\n",
        "        loss, acc = gradient_descent(X, y, learning_rate)\n",
        "        total_acc += acc*X.shape[0]\n",
        "        total_loss += loss*X.shape[0]\n",
        "        cnt += X.shape[0]\n",
        "    print('Train loss:', total_loss.numpy() / cnt, '- acc:', total_acc.numpy() / cnt)\n",
        "    test_loss, test_acc = evaluate(test_ds)\n",
        "    print('Test loss:', test_loss.numpy(), '- acc:', test_acc.numpy())\n",
        "    time.sleep(1) # sleep one second to flush out the progress bar\n"
      ]
    },
    {
      "source": [
        "### Bonus Question\n",
        "\n",
        "What was the accuracy of your perceptron? How does it compare to the random guess (Probability = 1/10)? Implement a MLP to see if there's any improvement."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PROVIDE YOUR ANSWER HERE"
      ]
    }
  ]
}